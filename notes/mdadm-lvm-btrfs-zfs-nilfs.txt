mdadm, LVM, BTRFS, ZFS and NILFS

mdadm
-----

https://raid.wiki.kernel.org/index.php/Linux_Raid
https://raid.wiki.kernel.org/index.php/Overview

$ sudo apt install mdadm

status: /proc/mdstat
    https://raid.wiki.kernel.org/index.php/Mdstat

configuration:
    /etc/mdadm/{mdadm.conf,mdadm.conf.d}
    or /etc/{mdadm.conf,mdadm.conf.d} if the above doesn't exist.

modes of operation:

    assemble
        -A, --assemble
    build
        -B, --build
    create
        -C, --create
    follow or monitor
        -F, --folow, --monitor
    grow or shrink
        -G, --grow
    incremental assembly
        -I, --incremental
    manage
        --manage    (optional)
            --add
            --re-add
            --add-spare
            --fail
            --remove
            --replace
    misc
        -Q, --query
        -D, --detail
        -E, --examine
        -R, --run
        -S, --stop
        -o, --readonly
        -w, --readwrite
    auto-detect (in-kernel auto-detect isn't recommended now)
        --auto-detect

Example:
    dd if=/dev/zero of=fs1.img bs=1M count=200
    for i in `seq 2 4`; do cp fs1.img fs$i.img; done
    for i in `seq 1 4`; do sudo losetup -f fs$i.img; done
    /sbin/losetup -l

    # RAID 0
    sudo mdadm --create --verbose /dev/md0 --level=raid0 --raid-devices=4 /dev/loop{0,1,2,3}
        # -l or --level, -n or --raid-devices
        # -c or --chunk, default is 512KB
    sudo mdadm --detail /dev/md0
    sudo mkfs -t xfs /dev/md0
    sudo mount /dev/md0 /mnt
    sudo xfs_info /mnt
        # mkfs.xfs automatically selects best stripe size and width of data section for RAID:
        #   -d su=<chunk_size(512k)>,sw=<num_of_data_disks>     # this way is easy to remember thus preferred
        #   or -d sunit=xxx,swidth=xxx
        #       where sunit * 512 bytes = RAID chunksize(512KB)
        #             swidth * 512 bytes = RAID chunksize(512KB) * num_of_data_disks
        #  Notice xfs_info has inconsistent sunit and swidth:
        #       sunit * bsize(4KB) = RAID chunksize(512KB)
        #       swidth * bsize(4KB) = RAID chunksize(512KB) * num_of_data_disks (not count mirror and parity disks)

    # RAID 1
    sudo mdadm --create --verbose /dev/md0 --level=raid1 --raid-devices=2 /dev/loop{0,1} \
        --spare-devices=2 /dev/loop{2,3}

    # RAID 5
    sudo mdadm --create --verbose /dev/md0 --level=raid5 --raid-devices=3 /dev/loop{0,1,2} \
        --spare-devices=1 /dev/loop3

    # RAID 6
    sudo mdadm --create --verbose /dev/md0 --level=raid6 --raid-devices=4 /dev/loop{0,1,2,3}

    # RAID 10
    sudo mdadm --create --verbose /dev/md0 --level=raid10 --layout=f2 --raid-devices=4 /dev/loop{0,1,2,3}
        # --layout=<n|o|f><copies>,  n: near, o: offset, f: far

    # Stop RAID device
    sudo mdadm --stop /dev/md0

    # Start RAID device
    sudo mdadm --assemble --scan
        # or explicitly: sudo mdadm --assemble /dev/md0 /dev/loop{0,1,2,3}
        #                sudo mdadm --assemble --scan --uuid=xxxxx

    # Save RAID configuration after the RAID has finished initialization(resync).
    sudo mdadm --detail --scan >> /etc/mdadm/mdadm.conf     # double check it !!!

    # Force-fail by software
    sudo mdadm /dev/md0 --fail /dev/loop3
    sudo mdadm /dev/md0 --remove /dev/loop3     # remove the faulty disk
    sudo mdadm /dev/md0 --add /dev/loop3        # add it back, the array recovers

    # Some optimizations for RAID:
    https://raid.wiki.kernel.org/index.php/Overview#Some_problem_solving_for_benchmarking


LVM 2
-----

$ sudo aptitude install lvm2
$ man lvm
$ man lvmcache
$ man lvmthin
$ man lvcreate
    --type=cache, cache-pool, error, linear, mirror, raid1, raid4,
           raid5_la, raid5_ls (= raid5), raid5_ra, raid5_rs,
           raid6_nc, raid6_nr, raid6_zr (= raid6), raid10, snapshot,
           striped, thin, thin-pool or zero

https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Logical_Volume_Manager_Administration/raid-scrub.html
data scrubbing:
    lvchange --syncaction {check|repair} vg/raid_lv


BTRFS
-----

$ sudo aptitude install btrfs-tools
$ sudo mkfs -t btrfs -d raid10 -m raid10 /dev/loop{0,1,2,3}
$ sudo blkid | grep /dev/loop
$ sudo btrfs filesystem show
$ sudo btrfs filesystem show /dev/loop0     # can be any of loop0/1/2/3
$ sudo mount -U $UUID /mnt
$ sudo mount /dev/loop0 /mnt                # can be any of loop0/1/2/3
    -o degraded,device=...,device=....,[subvol=rel-path-to-top-vol | subvolid=N]
       compress=xxx|compress-force=xxx,nodatacow,recovery,
       autodefrag   # works best for random writes on small files, not
                    # well-suited for large database workloads.

    chattr +C somefile  # no copy on write

    cp --reflink=auto  fileA fileB      # do CoW copy, fallback to standard copy if unsupported

$ btrfs filesystem df /mnt
$ sudo btrfs filesystem usage /mnt

$ sudo btrfs subvolume create /mnt/vol1
$ sudo btrfs subvolume list /mnt
$ sudo btrfs subvolume set-default <id> <path>
$ sudo btrfs subvolume show /mnt/vol1
$ sudo btrfs subvolume snapshot /mnt/vol1 /mnt/snapshot1
$ sudo btrfs subvolume snapshot -r /mnt/vol1 /mnt/snapshot1-readonly
$ sudo btrfs subvolume delete /mnt/vol1

    Concepts:
        mdadm/lvm/xfs       BTRFS           ZFS             comment
        =================   =============   =============   =============
        drive               drive           drive           physical disk
        LVM VG              fs or volume    storage pool
        LVM LE              chunk(1GB)                      RAID happens
        LVM LV + XFS        sub volume      fs

Suggested layout:
    top-level subvol
        +--- root subvol
        +--- home subvol
        \--- snapshots dir
                +--- root-YYYY-mm-dd snapshot
                +--- home-YYYY-mm-dd snapshot

Notice: read-only snapshot can't be moved because the '..' file in this sub
volume can't be changed, you can move the parent directory of this snapshot.


NILFS 2
-------

$ sudo apt install nilfs-tools
$ cat /etc/nilfs_cleanerd.conf

$ mkfs -t nilfs2 -L some-label fs.img
$ sudo mount fs.img /mnt
$ grep nilfs /proc/mounts
$ nilfs-tune -l /dev/loop0

$ sudo cp -r /etc /mnt
$ sudo cp -r /etc /mnt/a
$ lscp
$ sudo chcp ss /dev/loop0 2
$ mkdir snapshot-2
$ sudo mount -r -o cp=2 /dev/loop0 snapshot-2

$ sudo rmcp 1

Use "sudo nilfs-clean -p <TIME>" to reclaim space if space is ran out.

