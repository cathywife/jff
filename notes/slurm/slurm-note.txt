Run slurm:

    ### on host
    docker run -v `pwd`:/root/slurm --name slurm -h slurm -e container=docker --cap-add SYS_ADMIN \
        -dt debian:sid bash -c 'mount | grep /sys/fs/cgroup/ | awk "{print \$3}" | xargs -n 1 umount; mount -oremount,rw /sys/fs/cgroup; mkdir /sys/fs/cgroup/systemd; find /etc/systemd/system /lib/systemd/system -name "*tty*" -delete; exec /sbin/init'

    ### in container
    cd /root/slurm
    ./bootstrap-in-docker.sh

    ### on desktop
    apt install sview slurm-client


http://www.open-mpi.org/video/slurm/Slurm_EMC_Dec2012.pdf

    * resource: node -> NUMA board -> { memory, socket -> core -> hyperthread }
    * jobs: resource allocation requests
    * job steps: set of (typically parallel) tasks
    * partitions: job queues with limits and access controls
    * commands:
        * sbatch
        * salloc
        * srun -n8 a.out
        * sattach
        * sinfo
        * squeue
        * smap, sview
        * scontrol update NodeName=X State=[drain | resume] Reason=X
        * sacct,sstat, sreport
        * sacctmgr
        * sprio, sshare, sdiag
        * scancel, sbcast, srun_cr, strigger
        * slurmctld -Dcvvv  # run in foreground, purge state, verbose message
        * slurmd -Dcvvv
        * slurmd -C         # show node configuration

http://slurm.schedmd.com/cpu_management.html

    slurm.conf:
        NodeName
        PartitionMame
        FastSchedule
            0   configured processors == actual processors
            1   drain if configured processors > actual processors (default)
            2   NOT drain if configured processors > actual processors (for test)
        SelectType
            select/linear:      allocate whole node
            select/cons_res:    allocate CPU
        SelectTypeParameters

    srun/salloc/sbatch command line options:
        step 1: selection of nodes

            -p, --partition         ARG
            -F, --nodefile          ARG
            -w, --nodelist          ARG
            -x, --exclude           ARG

            -B, --extra-node-info   ARG
            -C, --constraint        ARG
            --contiguous
            --sockets-per-node      ARG
            --cores-per-socket      ARG
            --threads-per-core      ARG

            --exclusive             [=user] notice partition's "Shared" optiion takes precedence over the job's option
            -s, --share

            -N, --nodes             ARG     min and max number of nodes
            -n, --ntasks            ARG     number of tasks
            --ntasks-per-node       ARG     max
            --ntasks-per-socket     ARG     max
            --ntasks-per-core       ARG     max
            -c, --cpus-per-task     ARG
            --mincpus               ARG     min cpus per node

            --hint                  ARG

            -O, --overcommit
                for job allocation, allocate one CPU per node and --ntasks-per-* are ignored;
                for job step allocation, allow more than one task per CPU

            -Z, --no-allocate       bypass resource allocation, only for user slurm and root

        step 2: allocation of CPUs from the selected Nodes

            ...options in step 1...
            -m, --distribution      ARG

        step 3: distribution of tasks to the selected Nodes

            MaxTasksPerNode in slurm.conf

            -m, --distribution      ARG
            --ntasks-per-node       ARG
            --ntasks-per-core       ARG
            --ntasks-per-socket     ARG
            -r, --relative          ARG

        step 4: optional distribution and binding of tasks to CPUs within a node

            TaskPlugin, TaskPluginParam in slurm.conf

            ConstrainCores, TaskAffinity in cgroup.conf

            --cpu_bind              ARG
            --ntasks-per-core       ARG
            -m, --distribution      ARG

