Run slurm:

    ### on host
    docker run -v `pwd`:/root/slurm --name slurm -h slurm -e container=docker --cap-add SYS_ADMIN \
        -dt debian:sid bash -c 'mount | grep /sys/fs/cgroup/ | awk "{print \$3}" | xargs -n 1 umount; mount -oremount,rw /sys/fs/cgroup; mkdir /sys/fs/cgroup/systemd; find /etc/systemd/system /lib/systemd/system -name "*tty*" -delete; exec /sbin/init'

    ### in container
    cd /root/slurm
    ./bootstrap-in-docker.sh

    ### on desktop
    apt install sview slurm-client


http://www.open-mpi.org/video/slurm/Slurm_EMC_Dec2012.pdf

    * resource: node -> NUMA board -> { memory, socket -> core -> hyperthread }
    * jobs: resource allocation requests
    * job steps: set of (typically parallel) tasks
    * partitions: job queues with limits and access controls
    * commands:
        * sbatch
        * salloc
        * srun -n8 a.out
        * sattach
        * sinfo
        * squeue
        * smap, sview
        * scontrol update NodeName=X State=[drain | resume] Reason=X
        * sacct,sstat, sreport
        * sacctmgr
        * sprio, sshare, sdiag
        * scancel, sbcast, srun_cr, strigger
        * slurmctld -Dcvvv  # run in foreground, purge state, verbose message
        * slurmd -Dcvvv
        * slurmd -C         # show node configuration

http://slurm.schedmd.com/cpu_management.html

    slurm.conf:
        NodeName
        PartitionMame
        FastSchedule
            0   configured processors == actual processors
            1   drain if configured processors > actual processors (default)
            2   NOT drain if configured processors > actual processors (for test)
        SelectType
            select/linear:      allocate whole node
            select/cons_res:    allocate CPU
        SelectTypeParameters

    srun/salloc/sbatch command line options:
        step 1: selection of nodes

            -p, --partition         ARG
            -F, --nodefile          ARG
            -w, --nodelist          ARG
            -x, --exclude           ARG

            -C, --constraint        ARG
            --contiguous
            -B, --extra-node-info   ARG     combined shortcut for --sockets-per-node, --cores-per-socket, --threads-per-core
            --sockets-per-node      ARG
            --cores-per-socket      ARG
            --threads-per-core      ARG

            --exclusive             [=user] notice partition's "Shared" optiion takes precedence over the job's option
            -s, --share                     used with partition option "Shared=YES"

            -N, --nodes             ARG     min and max number of nodes
            -n, --ntasks            ARG     number of tasks
            -c, --cpus-per-task     ARG
            --ntasks-per-node       ARG     max
            --ntasks-per-socket     ARG     max
            --ntasks-per-core       ARG     max
            --mincpus               ARG     min cpus per node

            --mem                   ARG
            --mem-per-cpu           ARG

            --hint  compute_bound           use all cores in each socket
                    memory_bound            use only one socket in each socket
                    [no]multithread         [don't] use extra threads with in-core multi-threading

            --core-spec             ARG     count of cores to reserve for system use
            --thread-spec           ARG     count of threads to reserve for system use (future)

            -O, --overcommit
                for job allocation, allocate one CPU per node and --ntasks-per-* are ignored;
                for job step allocation, allow more than one task per CPU

            -Z, --no-allocate       bypass resource allocation, only for user slurm and root

        step 2: allocation of CPUs from the selected Nodes

            ...options in step 1...
            -m, --distribution      ARG

        step 3: distribution of tasks to the selected Nodes

            MaxTasksPerNode in slurm.conf

            -m, --distribution      ARG
            --ntasks-per-node       ARG
            --ntasks-per-core       ARG
            --ntasks-per-socket     ARG
            -r, --relative          ARG

        step 4: optional distribution and binding of tasks to CPUs within a node

            TaskPlugin, TaskPluginParam in slurm.conf

            ConstrainCores, TaskAffinity in cgroup.conf

            --cpu_bind              ARG
            --ntasks-per-core       ARG
            -m, --distribution      ARG

http://slurm.schedmd.com/quickstart_admin.html

    * same user/group/uid/grid across all nodes;
    * copy /etc/munge/munge.key to all nodes;
    * !!! slurmctld flushes state to disk every 5 seconds, state may lose;
    * !!! master and backup slurmctld require to share storage for "StateSaveLocation".
    * !!! Slurm daemons will support RPCs and state files from the two
      previous minor releases (e.g. a version 15.08.x SlurmDBD will
      support slurmctld daemons and commands with a version of 14.03.x
      or 14.11.x). This means that upgrading at least once each year is
      recommended. Otherwise, intermediate upgrades will be required to
      preserve state information.
    * !!! If the SlurmDBD daemon is used, it must be at the same or higher
      minor release number as the Slurmctld daemons. In other words, when
      changing the version to a higher release number (e.g from 14.11.x to
      15.08.x) always upgrade the SlurmDBD daemon first.
    * upgrade order: upgrade slurmdbd -> shutdown slurmctld -> upgrade slurmd -> upgrade slurmctld
    * rolling upgrade: upgrade slurmdbd -> upgrade slurmctld -> rolling upgrade slurmd, notice "SlurmdTimeout".

http://slurm.schedmd.com/accounting.html

    * Only lowercase usernames are supported.
